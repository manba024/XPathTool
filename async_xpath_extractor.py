#!/usr/bin/env python3
"""
å¼‚æ­¥XPathæå–å·¥å…· - ä½¿ç”¨LLMæ™ºèƒ½æå–ç½‘é¡µå…ƒç´ çš„XPath
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
import json
import sys
import os
from urllib.parse import urljoin, urlparse
import re
import time
from typing import Dict, List, Any, Optional, Tuple


class AsyncXPathExtractor:
    def __init__(self, api_key=None, api_base=None, model="Pro/deepseek-ai/DeepSeek-R1", 
                 max_http_concurrent=20, max_llm_concurrent=5, max_global_concurrent=50,
                 request_timeout=30, max_tokens=1000, temperature=0.1, connection_pool_size=100):
        """
        åˆå§‹åŒ–å¼‚æ­¥XPathæå–å™¨
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡SILICONFLOW_API_KEYè·å–ï¼‰
            api_base: APIåŸºç¡€URLï¼ˆé»˜è®¤ä½¿ç”¨ç¡…åŸºæµåŠ¨ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            max_http_concurrent: HTTPè¯·æ±‚æœ€å¤§å¹¶å‘æ•°
            max_llm_concurrent: LLM APIè°ƒç”¨æœ€å¤§å¹¶å‘æ•°
            max_global_concurrent: å…¨å±€æœ€å¤§å¹¶å‘æ•°
            request_timeout: HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_tokens: LLMè¾“å‡ºæœ€å¤§tokenæ•°
            temperature: LLMæ¸©åº¦å‚æ•°
            connection_pool_size: HTTPè¿æ¥æ± å¤§å°
        """
        self.api_key = api_key or os.getenv('SILICONFLOW_API_KEY')
        self.api_base = api_base or "https://api.siliconflow.cn/v1"
        self.model = model
        
        # é…ç½®å‚æ•°
        self.max_http_concurrent = max_http_concurrent
        self.max_llm_concurrent = max_llm_concurrent
        self.max_global_concurrent = max_global_concurrent
        self.request_timeout = request_timeout
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.connection_pool_size = connection_pool_size
        
        if not self.api_key:
            print("è­¦å‘Šï¼šæœªè®¾ç½®APIå¯†é’¥ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ SILICONFLOW_API_KEY æˆ–åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥api_keyå‚æ•°")
            return
        
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.api_base
        )
        
        # å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        self.http_semaphore = asyncio.Semaphore(max_http_concurrent)  # HTTPè¯·æ±‚å¹¶å‘
        self.llm_semaphore = asyncio.Semaphore(max_llm_concurrent)    # LLM APIå¹¶å‘
        self.global_semaphore = asyncio.Semaphore(max_global_concurrent) # å…¨å±€å¹¶å‘æ§åˆ¶
    
    async def fetch_webpage_async(self, session: aiohttp.ClientSession, url: str) -> Tuple[str, str]:
        """
        å¼‚æ­¥è·å–ç½‘é¡µå†…å®¹
        
        Args:
            session: aiohttpä¼šè¯
            url: ç›®æ ‡URL
            
        Returns:
            tuple: (html_content, cleaned_html)
        """
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        async with self.http_semaphore:
            try:
                async with session.get(url, headers=headers, timeout=self.request_timeout) as response:
                    response.raise_for_status()
                    html_content = await response.text()
                    
                    # ä½¿ç”¨BeautifulSoupæ¸…ç†å’Œæ ¼å¼åŒ–HTML
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # è·å–æ¸…ç†åçš„HTML
                    cleaned_html = str(soup)
                    
                    return html_content, cleaned_html
                    
            except Exception as e:
                raise Exception(f"è·å–ç½‘é¡µå¤±è´¥: {str(e)}")
    
    def create_dom_summary(self, html_content: str) -> str:
        """
        åˆ›å»ºDOMç»“æ„æ‘˜è¦ï¼Œå‡å°‘LLMè¾“å…¥é•¿åº¦
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            str: DOMç»“æ„æ‘˜è¦
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–å…³é”®ç»“æ„ä¿¡æ¯
        structure_info = []
        
        # æå–title
        title = soup.find('title')
        if title:
            structure_info.append(f"<title>{title.get_text().strip()}</title>")
        
        # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        for tag in soup.find_all(['h1', 'h2', 'h3', 'article', 'main', 'div'], limit=50):
            tag_info = f"<{tag.name}"
            
            # æ·»åŠ é‡è¦å±æ€§
            important_attrs = ['id', 'class', 'data-*']
            for attr in tag.attrs:
                if attr in ['id', 'class'] or attr.startswith('data-'):
                    tag_info += f' {attr}="{" ".join(tag.attrs[attr]) if isinstance(tag.attrs[attr], list) else tag.attrs[attr]}"'
            
            tag_info += ">"
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆæˆªæ–­ï¼‰
            text = tag.get_text().strip()
            if text:
                text = re.sub(r'\s+', ' ', text)[:100]
                tag_info += text
                if len(tag.get_text().strip()) > 100:
                    tag_info += "..."
            
            tag_info += f"</{tag.name}>"
            structure_info.append(tag_info)
        
        return "\n".join(structure_info)
    
    async def extract_xpath_with_llm_async(self, html_content: str, target_elements: List[str]) -> Dict[str, str]:
        """
        å¼‚æ­¥ä½¿ç”¨LLMæå–XPath
        
        Args:
            html_content: HTMLå†…å®¹
            target_elements: è¦æå–çš„å…ƒç´ åˆ—è¡¨ (å¦‚: ["æ ‡é¢˜", "æ­£æ–‡"])
            
        Returns:
            dict: å…ƒç´ åç§°åˆ°XPathçš„æ˜ å°„
        """
        dom_summary = self.create_dom_summary(html_content)
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹HTMLç»“æ„ï¼Œä¸ºæŒ‡å®šçš„å…ƒç´ æå–å‡†ç¡®çš„XPathé€‰æ‹©å™¨ã€‚

HTMLç»“æ„æ‘˜è¦ï¼š
{dom_summary}

éœ€è¦æå–çš„å…ƒç´ ï¼š{', '.join(target_elements)}

è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«æ¯ä¸ªå…ƒç´ çš„XPathï¼š
{{
    "å…ƒç´ å": "xpathè¡¨è¾¾å¼",
    ...
}}

è¦æ±‚ï¼š
1. XPathåº”è¯¥å°½å¯èƒ½ç²¾ç¡®å’Œç¨³å®š
2. ä¼˜å…ˆä½¿ç”¨idã€classç­‰ç¨³å®šå±æ€§
3. é¿å…ä½¿ç”¨ç»å¯¹ä½ç½®è·¯å¾„
4. è€ƒè™‘å…ƒç´ çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡

è¯·åªè¿”å›JSONï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜ã€‚
"""

        async with self.llm_semaphore:
            try:
                if not hasattr(self, 'async_client'):
                    raise Exception("APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
                    
                response = await self.async_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µåˆ†æä¸“å®¶ï¼Œæ“…é•¿æå–DOMå…ƒç´ çš„XPathé€‰æ‹©å™¨ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                result_text = response.choices[0].message.content.strip()
                
                # å°è¯•è§£æJSON
                try:
                    return json.loads(result_text)
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯æ ‡å‡†JSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                    json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
                    if json_match:
                        return json.loads(json_match.group())
                    else:
                        raise Exception("LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                        
            except Exception as e:
                raise Exception(f"LLMåˆ†æå¤±è´¥: {str(e)}")
    
    def validate_xpath(self, html_content: str, xpath_dict: Dict[str, str]) -> Dict[str, Dict[str, Any]]:
        """
        éªŒè¯XPathçš„æœ‰æ•ˆæ€§ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œå› ä¸ºlxmlæ˜¯åŒæ­¥çš„ï¼‰
        
        Args:
            html_content: HTMLå†…å®¹
            xpath_dict: XPathå­—å…¸
            
        Returns:
            dict: éªŒè¯ç»“æœ
        """
        from lxml import html, etree
        
        try:
            tree = html.fromstring(html_content)
            results = {}
            
            for element_name, xpath in xpath_dict.items():
                try:
                    elements = tree.xpath(xpath)
                    if elements:
                        # è·å–å…ƒç´ æ–‡æœ¬å†…å®¹
                        if hasattr(elements[0], 'text_content'):
                            content = elements[0].text_content().strip()
                        else:
                            content = str(elements[0]).strip()
                        
                        results[element_name] = {
                            "xpath": xpath,
                            "found": True,
                            "content": content[:200] + "..." if len(content) > 200 else content,
                            "element_count": len(elements)
                        }
                    else:
                        results[element_name] = {
                            "xpath": xpath,
                            "found": False,
                            "content": None,
                            "element_count": 0
                        }
                except Exception as e:
                    results[element_name] = {
                        "xpath": xpath,
                        "found": False,
                        "content": None,
                        "error": str(e)
                    }
            
            return results
            
        except Exception as e:
            raise Exception(f"XPathéªŒè¯å¤±è´¥: {str(e)}")
    
    async def extract_xpath_async(self, url: str, target_elements: List[str]) -> Dict[str, Any]:
        """
        å¼‚æ­¥ä¸»è¦æå–æ–¹æ³•
        
        Args:
            url: ç›®æ ‡URL
            target_elements: è¦æå–çš„å…ƒç´ åˆ—è¡¨
            
        Returns:
            dict: å®Œæ•´çš„æå–ç»“æœ
        """
        async with self.global_semaphore:
            start_time = time.time()
            
            print(f"æ­£åœ¨åˆ†æURL: {url}")
            print(f"ç›®æ ‡å…ƒç´ : {', '.join(target_elements)}")
            print("-" * 50)
            
            # åˆ›å»ºaiohttpä¼šè¯ï¼Œé…ç½®è¿æ¥æ± 
            connector = aiohttp.TCPConnector(limit=self.connection_pool_size)
            async with aiohttp.ClientSession(connector=connector) as session:
                try:
                    # 1. è·å–ç½‘é¡µå†…å®¹
                    raw_html, cleaned_html = await self.fetch_webpage_async(session, url)
                    print("âœ“ ç½‘é¡µå†…å®¹è·å–æˆåŠŸ")
                    
                    # 2. ä½¿ç”¨LLMæå–XPath
                    xpath_dict = await self.extract_xpath_with_llm_async(cleaned_html, target_elements)
                    print("âœ“ XPathæå–å®Œæˆ")
                    
                    # 3. éªŒè¯XPathï¼ˆåŒæ­¥æ“ä½œï¼Œä½†å¾ˆå¿«ï¼‰
                    validation_results = self.validate_xpath(cleaned_html, xpath_dict)
                    print("âœ“ XPathéªŒè¯å®Œæˆ")
                    
                    processing_time = time.time() - start_time
                    
                    return {
                        "url": url,
                        "target_elements": target_elements,
                        "xpath_results": validation_results,
                        "processing_time": round(processing_time, 2),
                        "status": "success",
                        "summary": {
                            "total_elements": len(target_elements),
                            "successful_extractions": sum(1 for r in validation_results.values() if r.get("found", False)),
                            "failed_extractions": sum(1 for r in validation_results.values() if not r.get("found", False))
                        }
                    }
                    
                except Exception as e:
                    processing_time = time.time() - start_time
                    return {
                        "url": url,
                        "target_elements": target_elements,
                        "status": "error",
                        "error": str(e),
                        "processing_time": round(processing_time, 2),
                        "xpath_results": {},
                        "summary": {
                            "total_elements": len(target_elements),
                            "successful_extractions": 0,
                            "failed_extractions": len(target_elements)
                        }
                    }


async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python async_xpath_extractor.py <URL> <å…ƒç´ 1> [å…ƒç´ 2] [å…ƒç´ 3] ...")
        print("ç¤ºä¾‹: python async_xpath_extractor.py https://example.com æ ‡é¢˜ æ­£æ–‡")
        sys.exit(1)
    
    url = sys.argv[1]
    target_elements = sys.argv[2:]
    
    # åˆå§‹åŒ–æå–å™¨ï¼ˆéœ€è¦é…ç½®APIå¯†é’¥ï¼‰
    extractor = AsyncXPathExtractor()
    
    try:
        # æ‰§è¡Œæå–
        results = await extractor.extract_xpath_async(url, target_elements)
        
        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("å¼‚æ­¥XPath æå–ç»“æœ")
        print("=" * 60)
        print(f"URL: {results['url']}")
        print(f"æå–æˆåŠŸ: {results['summary']['successful_extractions']}/{results['summary']['total_elements']}")
        print(f"å¤„ç†æ—¶é—´: {results.get('processing_time', 0):.2f}ç§’")
        print("-" * 60)
        
        # æŒ‰ç”¨æˆ·è¦æ±‚çš„æ ¼å¼è¾“å‡ºï¼šURL + æ•°æ®åç§° + XPath
        print("\nğŸ“‹ æå–ç»“æœæ‘˜è¦:")
        for element_name, result in results['xpath_results'].items():
            if result['found']:
                print(f"{results['url']} + {element_name} + {result['xpath']}")
        
        print("\nğŸ“ è¯¦ç»†ä¿¡æ¯:")
        for element_name, result in results['xpath_results'].items():
            print(f"\nã€{element_name}ã€‘")
            print(f"XPath: {result['xpath']}")
            print(f"çŠ¶æ€: {'âœ“ æˆåŠŸ' if result['found'] else 'âœ— å¤±è´¥'}")
            
            if result['found']:
                print(f"å†…å®¹é¢„è§ˆ: {result['content']}")
                print(f"åŒ¹é…å…ƒç´ æ•°: {result['element_count']}")
            elif 'error' in result:
                print(f"é”™è¯¯: {result['error']}")
        
        print("\n" + "=" * 60)
        
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main_async())