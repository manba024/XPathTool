#!/usr/bin/env python3
"""
XPathæå–å·¥å…· - ä½¿ç”¨LLMæ™ºèƒ½æå–ç½‘é¡µå…ƒç´ çš„XPath
"""

import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import json
import sys
import os
from urllib.parse import urljoin, urlparse
import re


class XPathExtractor:
    def __init__(self, api_key=None, api_base=None, model="Pro/deepseek-ai/DeepSeek-R1"):
        """
        åˆå§‹åŒ–XPathæå–å™¨
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡SILICONFLOW_API_KEYè·å–ï¼‰
            api_base: APIåŸºç¡€URLï¼ˆé»˜è®¤ä½¿ç”¨ç¡…åŸºæµåŠ¨ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.api_key = api_key or os.getenv('SILICONFLOW_API_KEY')
        self.api_base = api_base or "https://api.siliconflow.cn/v1"
        self.model = model
        
        if not self.api_key:
            print("è­¦å‘Šï¼šæœªè®¾ç½®APIå¯†é’¥ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ SILICONFLOW_API_KEY æˆ–åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥api_keyå‚æ•°")
            return
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.api_base
        )
    
    def fetch_webpage(self, url):
        """
        è·å–ç½‘é¡µå†…å®¹
        
        Args:
            url: ç›®æ ‡URL
            
        Returns:
            tuple: (html_content, cleaned_html)
        """
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            
            # ä½¿ç”¨BeautifulSoupæ¸…ç†å’Œæ ¼å¼åŒ–HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–æ¸…ç†åçš„HTML
            cleaned_html = str(soup)
            print(cleaned_html)
            
            return response.text, cleaned_html
            
        except Exception as e:
            raise Exception(f"è·å–ç½‘é¡µå¤±è´¥: {str(e)}")
    
    def create_dom_summary(self, html_content):
        """
        åˆ›å»ºDOMç»“æ„æ‘˜è¦ï¼Œå‡å°‘LLMè¾“å…¥é•¿åº¦
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            str: DOMç»“æ„æ‘˜è¦
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–å…³é”®ç»“æ„ä¿¡æ¯
        structure_info = []
        
        # æå–title
        title = soup.find('title')
        if title:
            structure_info.append(f"<title>{title.get_text().strip()}</title>")
        
        # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        for tag in soup.find_all(['h1', 'h2', 'h3', 'article', 'main', 'div'], limit=50):
            tag_info = f"<{tag.name}"
            
            # æ·»åŠ é‡è¦å±æ€§
            important_attrs = ['id', 'class', 'data-*']
            for attr in tag.attrs:
                if attr in ['id', 'class'] or attr.startswith('data-'):
                    tag_info += f' {attr}="{" ".join(tag.attrs[attr]) if isinstance(tag.attrs[attr], list) else tag.attrs[attr]}"'
            
            tag_info += ">"
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆæˆªæ–­ï¼‰
            text = tag.get_text().strip()
            if text:
                text = re.sub(r'\s+', ' ', text)[:100]
                tag_info += text
                if len(tag.get_text().strip()) > 100:
                    tag_info += "..."
            
            tag_info += f"</{tag.name}>"
            structure_info.append(tag_info)
        
        return "\n".join(structure_info)
    
    def extract_xpath_with_llm(self, html_content, target_elements):
        """
        ä½¿ç”¨LLMæå–XPath
        
        Args:
            html_content: HTMLå†…å®¹
            target_elements: è¦æå–çš„å…ƒç´ åˆ—è¡¨ (å¦‚: ["æ ‡é¢˜", "æ­£æ–‡"])
            
        Returns:
            dict: å…ƒç´ åç§°åˆ°XPathçš„æ˜ å°„
        """
        dom_summary = self.create_dom_summary(html_content)
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹HTMLç»“æ„ï¼Œä¸ºæŒ‡å®šçš„å…ƒç´ æå–å‡†ç¡®çš„XPathé€‰æ‹©å™¨ã€‚

HTMLç»“æ„æ‘˜è¦ï¼š
{dom_summary}

éœ€è¦æå–çš„å…ƒç´ ï¼š{', '.join(target_elements)}

è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«æ¯ä¸ªå…ƒç´ çš„XPathï¼š
{{
    "å…ƒç´ å": "xpathè¡¨è¾¾å¼",
    ...
}}

è¦æ±‚ï¼š
1. XPathåº”è¯¥å°½å¯èƒ½ç²¾ç¡®å’Œç¨³å®š
2. ä¼˜å…ˆä½¿ç”¨idã€classç­‰ç¨³å®šå±æ€§
3. é¿å…ä½¿ç”¨ç»å¯¹ä½ç½®è·¯å¾„
4. è€ƒè™‘å…ƒç´ çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡

è¯·åªè¿”å›JSONï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜ã€‚
"""

        try:
            if not hasattr(self, 'client'):
                raise Exception("APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
                
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µåˆ†æä¸“å®¶ï¼Œæ“…é•¿æå–DOMå…ƒç´ çš„XPathé€‰æ‹©å™¨ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # å°è¯•è§£æJSON
            try:
                return json.loads(result_text)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯æ ‡å‡†JSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                else:
                    raise Exception("LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
        except Exception as e:
            raise Exception(f"LLMåˆ†æå¤±è´¥: {str(e)}")
    
    def validate_xpath(self, html_content, xpath_dict):
        """
        éªŒè¯XPathçš„æœ‰æ•ˆæ€§
        
        Args:
            html_content: HTMLå†…å®¹
            xpath_dict: XPathå­—å…¸
            
        Returns:
            dict: éªŒè¯ç»“æœ
        """
        from lxml import html, etree
        
        try:
            tree = html.fromstring(html_content)
            results = {}
            
            for element_name, xpath in xpath_dict.items():
                try:
                    elements = tree.xpath(xpath)
                    if elements:
                        # è·å–å…ƒç´ æ–‡æœ¬å†…å®¹
                        if hasattr(elements[0], 'text_content'):
                            content = elements[0].text_content().strip()
                        else:
                            content = str(elements[0]).strip()
                        
                        results[element_name] = {
                            "xpath": xpath,
                            "found": True,
                            "content": content[:200] + "..." if len(content) > 200 else content,
                            "element_count": len(elements)
                        }
                    else:
                        results[element_name] = {
                            "xpath": xpath,
                            "found": False,
                            "content": None,
                            "element_count": 0
                        }
                except Exception as e:
                    results[element_name] = {
                        "xpath": xpath,
                        "found": False,
                        "content": None,
                        "error": str(e)
                    }
            
            return results
            
        except Exception as e:
            raise Exception(f"XPathéªŒè¯å¤±è´¥: {str(e)}")
    
    def extract_xpath(self, url, target_elements):
        """
        ä¸»è¦æå–æ–¹æ³•
        
        Args:
            url: ç›®æ ‡URL
            target_elements: è¦æå–çš„å…ƒç´ åˆ—è¡¨
            
        Returns:
            dict: å®Œæ•´çš„æå–ç»“æœ
        """
        print(f"æ­£åœ¨åˆ†æURL: {url}")
        print(f"ç›®æ ‡å…ƒç´ : {', '.join(target_elements)}")
        print("-" * 50)
        
        # 1. è·å–ç½‘é¡µå†…å®¹
        raw_html, cleaned_html = self.fetch_webpage(url)
        print("âœ“ ç½‘é¡µå†…å®¹è·å–æˆåŠŸ")
        
        # 2. ä½¿ç”¨LLMæå–XPath
        xpath_dict = self.extract_xpath_with_llm(cleaned_html, target_elements)
        print("âœ“ XPathæå–å®Œæˆ")
        
        # 3. éªŒè¯XPath
        validation_results = self.validate_xpath(cleaned_html, xpath_dict)
        print("âœ“ XPathéªŒè¯å®Œæˆ")
        
        return {
            "url": url,
            "target_elements": target_elements,
            "xpath_results": validation_results,
            "summary": {
                "total_elements": len(target_elements),
                "successful_extractions": sum(1 for r in validation_results.values() if r.get("found", False)),
                "failed_extractions": sum(1 for r in validation_results.values() if not r.get("found", False))
            }
        }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python xpath_extractor.py <URL> <å…ƒç´ 1> [å…ƒç´ 2] [å…ƒç´ 3] ...")
        print("ç¤ºä¾‹: python xpath_extractor.py https://example.com æ ‡é¢˜ æ­£æ–‡")
        sys.exit(1)
    
    url = sys.argv[1]
    target_elements = sys.argv[2:]
    
    # åˆå§‹åŒ–æå–å™¨ï¼ˆéœ€è¦é…ç½®APIå¯†é’¥ï¼‰
    extractor = XPathExtractor()
    
    try:
        # æ‰§è¡Œæå–
        results = extractor.extract_xpath(url, target_elements)
        
        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("XPath æå–ç»“æœ")
        print("=" * 60)
        print(f"URL: {results['url']}")
        print(f"æå–æˆåŠŸ: {results['summary']['successful_extractions']}/{results['summary']['total_elements']}")
        print("-" * 60)
        
        # æŒ‰ç”¨æˆ·è¦æ±‚çš„æ ¼å¼è¾“å‡ºï¼šURL + æ•°æ®åç§° + XPath
        print("\nğŸ“‹ æå–ç»“æœæ‘˜è¦:")
        for element_name, result in results['xpath_results'].items():
            if result['found']:
                print(f"{results['url']} + {element_name} + {result['xpath']}")
        
        print("\nğŸ“ è¯¦ç»†ä¿¡æ¯:")
        for element_name, result in results['xpath_results'].items():
            print(f"\nã€{element_name}ã€‘")
            print(f"XPath: {result['xpath']}")
            print(f"çŠ¶æ€: {'âœ“ æˆåŠŸ' if result['found'] else 'âœ— å¤±è´¥'}")
            
            if result['found']:
                print(f"å†…å®¹é¢„è§ˆ: {result['content']}")
                print(f"åŒ¹é…å…ƒç´ æ•°: {result['element_count']}")
            elif 'error' in result:
                print(f"é”™è¯¯: {result['error']}")
        
        print("\n" + "=" * 60)
        
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()